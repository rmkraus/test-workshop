configs:
  server_config_dual_gpu:
    content: "summarization:\n  enable: true\n  method: \"batch\"\n  llm:\n    model:\
      \ meta/llama-3.1-8b-instruct\n    base_url: http://llama-3-1-8b-instruct:8000/v1\n\
      \    max_tokens: 2048\n    temperature: 0.2\n    top_p: 0.7\n  embedding:\n\
      \    model: \"nvidia/llama-3.2-nv-embedqa-1b-v2\"\n    base_url: http://llama-3-2-nv-embedqa:8000/v1\n\
      \  params:\n    batch_size: 5\n    batch_max_concurrency: 20\n  prompts:\n \
      \   caption: \"Write a concise and clear dense caption for the provided warehouse\
      \ video, focusing on irregular or hazardous events such as boxes falling, workers\
      \ not wearing PPE, workers falling, workers taking photographs, workers chitchatting,\
      \ forklift stuck, etc. Start and end each sentence with a time stamp.\"\n  \
      \  caption_summarization: \"You should summarize the following events of a warehouse\
      \ in the format start_time:end_time:caption. For start_time and end_time use\
      \ . to seperate seconds, minutes, hours. If during a time segment only regular\
      \ activities happen, then ignore them, else note any irregular activities in\
      \ detail. The output should be bullet points in the format start_time:end_time:\
      \ detailed_event_description. Don't return anything else except the bullet points.\"\
      \n    summary_aggregation: \"You are a warehouse monitoring system. Given the\
      \ caption in the form start_time:end_time: caption, Aggregate the following\
      \ captions in the format start_time:end_time:event_description. If the event_description\
      \ is the same as another event_description, aggregate the captions in the format\
      \ start_time1:end_time1,...,start_timek:end_timek:event_description. If any\
      \ two adjacent end_time1 and start_time2 is within a few tenths of a second,\
      \ merge the captions in the format start_time1:end_time2. The output should\
      \ only contain bullet points.  Cluster the output into Unsafe Behavior, Operational\
      \ Inefficiencies, Potential Equipment Damage and Unauthorized Personnel\"\n\n\
      chat:\n  rag: graph-rag # graph-rag or vector-rag #If using a small LLM model,\
      \ vector-rag is recommended.\n  params:\n    batch_size: 5\n    top_k: 5\n \
      \ llm:\n    model: meta/llama-3.1-8b-instruct\n    base_url: http://llama-3-1-8b-instruct:8000/v1\n\
      \    max_tokens: 2048\n    temperature: 0.2\n    top_p: 0.7\n  embedding:\n\
      \    model: \"nvidia/llama-3.2-nv-embedqa-1b-v2\"\n    base_url: http://llama-3-2-nv-embedqa:8000/v1\n\
      \  reranker:\n    model: \"nvidia/llama-3.2-nv-rerankqa-1b-v2\"\n    base_url:\
      \ http://llama-3.2-nv-rerankqa:8000/v1\n\nnotification:\n  enable: true\n  endpoint:\
      \ \"http://host.docker.internal:60000/via-alert-callback\"\n  llm:\n    model:\
      \ meta/llama-3.1-8b-instruct\n    base_url: http://llama-3-1-8b-instruct:8000/v1\n\
      \    max_tokens: 2048\n    temperature: 0.2\n    top_p: 0.7\n"
  server_config_quad_gpu:
    content: "summarization:\n  enable: true\n  method: \"batch\"\n  llm:\n    model:\
      \ \"meta/llama-3.1-70b-instruct\"\n    base_url: http://llama-3-1-70b-instruct:8000/v1\n\
      \    max_tokens: 2048\n    temperature: 0.2\n    top_p: 0.7\n  embedding:\n\
      \    model: \"nvidia/llama-3.2-nv-embedqa-1b-v2\"\n    base_url: http://llama-3-2-nv-embedqa:8000/v1\n\
      \  params:\n    batch_size: 5\n    batch_max_concurrency: 20\n  prompts:\n \
      \   caption: \"Write a concise and clear dense caption for the provided warehouse\
      \ video, focusing on irregular or hazardous events such as boxes falling, workers\
      \ not wearing PPE, workers falling, workers taking photographs, workers chitchatting,\
      \ forklift stuck, etc. Start and end each sentence with a time stamp.\"\n  \
      \  caption_summarization: \"You should summarize the following events of a warehouse\
      \ in the format start_time:end_time:caption. For start_time and end_time use\
      \ . to seperate seconds, minutes, hours. If during a time segment only regular\
      \ activities happen, then ignore them, else note any irregular activities in\
      \ detail. The output should be bullet points in the format start_time:end_time:\
      \ detailed_event_description. Don't return anything else except the bullet points.\"\
      \n    summary_aggregation: \"You are a warehouse monitoring system. Given the\
      \ caption in the form start_time:end_time: caption, Aggregate the following\
      \ captions in the format start_time:end_time:event_description. If the event_description\
      \ is the same as another event_description, aggregate the captions in the format\
      \ start_time1:end_time1,...,start_timek:end_timek:event_description. If any\
      \ two adjacent end_time1 and start_time2 is within a few tenths of a second,\
      \ merge the captions in the format start_time1:end_time2. The output should\
      \ only contain bullet points.  Cluster the output into Unsafe Behavior, Operational\
      \ Inefficiencies, Potential Equipment Damage and Unauthorized Personnel\"\n\n\
      chat:\n  rag: graph-rag # graph-rag or vector-rag #If using a small LLM model,\
      \ vector-rag is recommended.\n  params:\n    batch_size: 5\n    top_k: 5\n \
      \ llm:\n    model: \"meta/llama-3.1-70b-instruct\"\n    base_url: http://llama-3-1-70b-instruct:8000/v1\n\
      \    max_tokens: 2048\n    temperature: 0.2\n    top_p: 0.7\n  embedding:\n\
      \    model: \"nvidia/llama-3.2-nv-embedqa-1b-v2\"\n    base_url: http://llama-3-2-nv-embedqa:8000/v1\n\
      \  reranker:\n    model: \"nvidia/llama-3.2-nv-rerankqa-1b-v2\"\n    base_url:\
      \ http://llama-3.2-nv-rerankqa:8000/v1\n\nnotification:\n  enable: true\n  endpoint:\
      \ \"http://host.docker.internal:60000/via-alert-callback\"\n  llm:\n    model:\
      \ \"meta/llama-3.1-70b-instruct\"\n    base_url: http://llama-3-1-70b-instruct:8000/v1\n\
      \    max_tokens: 2048\n    temperature: 0.2\n    top_p: 0.7\n"
  server_config_single_gpu:
    content: "summarization:\n  enable: true\n  method: \"batch\"\n  llm:\n    model:\
      \ \"meta/llama-3.1-70b-instruct\"\n    base_url: \"https://integrate.api.nvidia.com/v1\"\
      \n    max_tokens: 2048\n    temperature: 0.2\n    top_p: 0.7\n  embedding:\n\
      \    model: \"nvidia/llama-3.2-nv-embedqa-1b-v2\"\n    base_url: \"https://integrate.api.nvidia.com/v1\"\
      \n  params:\n    batch_size: 5\n    batch_max_concurrency: 20\n  prompts:\n\
      \    caption: \"Write a concise and clear dense caption for the provided warehouse\
      \ video, focusing on irregular or hazardous events such as boxes falling, workers\
      \ not wearing PPE, workers falling, workers taking photographs, workers chitchatting,\
      \ forklift stuck, etc. Start and end each sentence with a time stamp.\"\n  \
      \  caption_summarization: \"You should summarize the following events of a warehouse\
      \ in the format start_time:end_time:caption. For start_time and end_time use\
      \ . to seperate seconds, minutes, hours. If during a time segment only regular\
      \ activities happen, then ignore them, else note any irregular activities in\
      \ detail. The output should be bullet points in the format start_time:end_time:\
      \ detailed_event_description. Don't return anything else except the bullet points.\"\
      \n    summary_aggregation: \"You are a warehouse monitoring system. Given the\
      \ caption in the form start_time:end_time: caption, Aggregate the following\
      \ captions in the format start_time:end_time:event_description. If the event_description\
      \ is the same as another event_description, aggregate the captions in the format\
      \ start_time1:end_time1,...,start_timek:end_timek:event_description. If any\
      \ two adjacent end_time1 and start_time2 is within a few tenths of a second,\
      \ merge the captions in the format start_time1:end_time2. The output should\
      \ only contain bullet points.  Cluster the output into Unsafe Behavior, Operational\
      \ Inefficiencies, Potential Equipment Damage and Unauthorized Personnel\"\n\n\
      chat:\n  rag: graph-rag # graph-rag or vector-rag #If using a small LLM model,\
      \ vector-rag is recommended.\n  params:\n    batch_size: 5\n    top_k: 5\n \
      \ llm:\n    model: \"meta/llama-3.1-70b-instruct\"\n    base_url: \"https://integrate.api.nvidia.com/v1\"\
      \n    max_tokens: 2048\n    temperature: 0.2\n    top_p: 0.7\n  embedding:\n\
      \    model: \"nvidia/llama-3.2-nv-embedqa-1b-v2\"\n    base_url: \"https://integrate.api.nvidia.com/v1\"\
      \n  reranker:\n    model: \"nvidia/llama-3.2-nv-rerankqa-1b-v2\"\n    base_url:\
      \ \"https://integrate.api.nvidia.com/v1\"\n\nnotification:\n  enable: true\n\
      \  endpoint: \"http://host.docker.internal:60000/via-alert-callback\"\n  llm:\n\
      \    model: \"meta/llama-3.1-70b-instruct\"\n    base_url: \"https://integrate.api.nvidia.com/v1\"\
      \n    max_tokens: 2048\n    temperature: 0.2\n    top_p: 0.7\n"
services:
  devx:
    env_file:
    - variables.env
    environment:
      NGC_API_KEY: ${NGC_API_KEY}
    image: ghcr.io/rmkraus/test-workshop/devx:main
    ports:
    - 8888:8888
    volumes:
    - ../test-workshop:/project:cached
    - /var/run/docker.sock:/var/run/docker.sock
  graph-db:
    environment:
      NEO4J_AUTH: ${GRAPH_DB_USERNAME:-neo4j}/${GRAPH_DB_PASSWORD:-password}
      NEO4J_PLUGINS: '["apoc"]'
    healthcheck:
      interval: 1s
      retries: 20
      start_period: 3s
      test: wget http://localhost:7474 || exit 1
      timeout: 10s
    hostname: graph-db
    image: neo4j:5.26.4
    ports:
    - ${GRAPH_DB_HTTP_PORT:-7474}:7474
    - ${GRAPH_DB_BOLT_PORT:-7687}:7687
    restart: always
  llama-3-1-70b-instruct:
    deploy:
      resources:
        reservations:
          devices:
          - capabilities:
            - gpu
            device_ids:
            - '2'
            - '3'
            driver: nvidia
    environment:
    - NGC_API_KEY=${NGC_API_KEY}
    - NIM_LOW_MEMORY_MODE=1
    - NIM_RELAX_MEM_CONSTRAINTS=1
    extra_hosts:
      host.docker.internal: host-gateway
    healthcheck:
      interval: 30s
      retries: 60
      start_period: 1800s
      test:
      - CMD
      - python3
      - -c
      - import http.client; conn = http.client.HTTPConnection('localhost', 8000);
        conn.request('GET', '/v1/health/ready'); response = conn.getresponse(); exit(0
        if response.status == 200 else 1)
      timeout: 10s
    hostname: llama-3-1-70b-instruct
    image: nvcr.io/nim/meta/llama-3.1-70b-instruct:1.3.3
    ports:
    - 8007:8000
    profiles:
    - local-deployment-quad-gpu
    shm_size: 16gb
    volumes:
    - via-nim-cache:/opt/nim/.cache
  llama-3-1-8b-instruct:
    deploy:
      resources:
        reservations:
          devices:
          - capabilities:
            - gpu
            device_ids:
            - '1'
            driver: nvidia
    environment:
    - NGC_API_KEY=${NGC_API_KEY}
    - NIM_LOW_MEMORY_MODE=1
    - NIM_RELAX_MEM_CONSTRAINTS=1
    extra_hosts:
      host.docker.internal: host-gateway
    healthcheck:
      interval: 30s
      retries: 60
      start_period: 1800s
      test:
      - CMD
      - python3
      - -c
      - import http.client; conn = http.client.HTTPConnection('localhost', 8000);
        conn.request('GET', '/v1/health/ready'); response = conn.getresponse(); exit(0
        if response.status == 200 else 1)
      timeout: 10s
    hostname: llama-3-1-8b-instruct
    image: nvcr.io/nim/meta/llama-3.1-8b-instruct:1.3.3
    ports:
    - 8007:8000
    profiles:
    - local-deployment-dual-gpu
    shm_size: 16gb
    volumes:
    - via-nim-cache:/opt/nim/.cache
  llama-3-2-nv-embedqa:
    deploy:
      resources:
        reservations:
          devices:
          - capabilities:
            - gpu
            device_ids:
            - '1'
            driver: nvidia
    environment:
    - NGC_API_KEY=${NGC_API_KEY}
    - NIM_SERVER_PORT=8000
    - NIM_MODEL_PROFILE=f7391ddbcb95b2406853526b8e489fedf20083a2420563ca3e65358ff417b10f
    extra_hosts:
      host.docker.internal: host-gateway
    healthcheck:
      interval: 30s
      retries: 60
      start_period: 1800s
      test:
      - CMD
      - curl
      - -f
      - http://localhost:8000/v1/health/ready
      timeout: 10s
    hostname: llama-3-2-nv-embedqa
    image: nvcr.io/nim/nvidia/llama-3.2-nv-embedqa-1b-v2:1.3.0
    ports:
    - 8006:8000
    profiles:
    - local-deployment-dual-gpu
    - local-deployment-quad-gpu
    shm_size: 16gb
    volumes:
    - via-nim-cache:/opt/nim/.cache
  llama-3.2-nv-rerankqa:
    deploy:
      resources:
        reservations:
          devices:
          - capabilities:
            - gpu
            device_ids:
            - '1'
            driver: nvidia
    environment:
    - NGC_API_KEY=${NGC_API_KEY}
    - NIM_SERVER_PORT=8000
    - NIM_MODEL_PROFILE=f7391ddbcb95b2406853526b8e489fedf20083a2420563ca3e65358ff417b10f
    extra_hosts:
      host.docker.internal: host-gateway
    healthcheck:
      interval: 30s
      retries: 60
      start_period: 1800s
      test:
      - CMD
      - curl
      - -f
      - http://localhost:8000/v1/health/ready
      timeout: 10s
    hostname: llama-3.2-nv-rerankqa
    image: nvcr.io/nim/nvidia/llama-3.2-nv-rerankqa-1b-v2:1.3.0
    ports:
    - 8005:8000
    profiles:
    - local-deployment-dual-gpu
    - local-deployment-quad-gpu
    shm_size: 16gb
    volumes:
    - via-nim-cache:/opt/nim/.cache
  via-server-dual-gpu:
    configs:
    - source: server_config_dual_gpu
      target: /opt/nvidia/via/default_config.yaml
    depends_on: &id001
    - graph-db
    deploy: &id002
      resources:
        reservations:
          devices:
          - capabilities:
            - gpu
            device_ids:
            - '0'
            driver: nvidia
    environment: &id003
      AZURE_OPENAI_API_KEY: ${AZURE_OPENAI_API_KEY:-}
      AZURE_OPENAI_ENDPOINT: ${AZURE_OPENAI_ENDPOINT:-}
      BACKEND_PORT: ${BACKEND_PORT:-8100}
      CA_RAG_EMBEDDINGS_DIMENSION: '500'
      DISABLE_CA_RAG: ${DISABLE_CA_RAG:-false}
      DISABLE_CV_PIPELINE: ${DISABLE_CV_PIPELINE:-true}
      DISABLE_FRONTEND: ${DISABLE_FRONTEND:-false}
      DISABLE_GUARDRAILS: ${DISABLE_GUARDRAILS:-false}
      ENABLE_AUDIO: ${ENABLE_AUDIO:-false}
      ENABLE_DENSE_CAPTION: ${ENABLE_DENSE_CAPTION:-}
      ENABLE_RIVA_SERVER_READINESS_CHECK: ${ENABLE_RIVA_SERVER_READINESS_CHECK:-}
      FORCE_CA_RAG_RESET: ${FORCE_CA_RAG_RESET:-}
      FORCE_SW_AV1_DECODER: ${FORCE_SW_AV1_DECODER:-}
      FRONTEND_PORT: ${FRONTEND_PORT:-9100}
      GDINO_INFERENCE_INTERVAL: ${GDINO_INFERENCE_INTERVAL:-}
      GDINO_MODEL_PATH: ${GDINO_MODEL_PATH:-}
      GRAPH_DB_PASSWORD: ${GRAPH_DB_PASSWORD:-password}
      GRAPH_DB_URI: ${GRAPH_DB_URI:-bolt://graph-db:7687}
      GRAPH_DB_USERNAME: ${GRAPH_DB_USERNAME:-neo4j}
      GRAPH_RAG_PROMPT_CONFIG: ${GRAPH_RAG_PROMPT_CONFIG:-}
      INSTALL_PROPRIETARY_CODECS: ${INSTALL_PROPRIETARY_CODECS:-false}
      MILVUS_DB_HOST: ${MILVUS_DB_HOST:-}
      MILVUS_DB_PORT: ${MILVUS_DB_PORT:-}
      MODEL_PATH: ${MODEL_PATH:-}
      NGC_API_KEY: ${NGC_API_KEY:-}
      NUM_CV_CHUNKS_PER_GPU: ${NUM_CV_CHUNKS_PER_GPU:-}
      NUM_GPUS: ${NUM_GPUS:-}
      NUM_VLM_PROCS: ${NUM_VLM_PROCS:-}
      NVIDIA_API_KEY: ${NGC_API_KEY:-}
      NVIDIA_VISIBLE_DEVICES: ${NVIDIA_VISIBLE_DEVICES:-all}
      NVILA_VIDEO_MAX_TILES: ${NVILA_VIDEO_MAX_TILES:-}
      NV_LLMG_CLIENT_ID: ${NV_LLMG_CLIENT_ID:-}
      NV_LLMG_CLIENT_SECRET: ${NV_LLMG_CLIENT_SECRET:-}
      OPENAI_API_KEY: ${OPENAI_API_KEY:-}
      OPENAI_API_VERSION: ${OPENAI_API_VERSION:-}
      RIVA_ASR_GRPC_PORT: ${RIVA_ASR_GRPC_PORT:-50051}
      RIVA_ASR_HTTP_PORT: ${RIVA_ASR_HTTP_PORT:-}
      RIVA_ASR_MODEL_NAME: ${RIVA_ASR_MODEL_NAME:-}
      RIVA_ASR_SERVER_API_KEY: ${RIVA_ASR_SERVER_API_KEY:-}
      RIVA_ASR_SERVER_FUNC_ID: ${RIVA_ASR_SERVER_FUNC_ID:-}
      RIVA_ASR_SERVER_IS_NIM: ${RIVA_ASR_SERVER_IS_NIM:-true}
      RIVA_ASR_SERVER_URI: ${RIVA_ASR_SERVER_URI:-parakeet-ctc-asr}
      RIVA_ASR_SERVER_USE_SSL: ${RIVA_ASR_SERVER_USE_SSL:-false}
      TRT_ENGINE_PATH: ${TRT_ENGINE_PATH:-}
      TRT_LLM_MEM_USAGE_FRACTION: ${TRT_LLM_MEM_USAGE_FRACTION:-}
      TRT_LLM_MODE: ${TRT_LLM_MODE:-}
      VIA_VLM_API_KEY: ${VIA_VLM_API_KEY:-}
      VIA_VLM_OPENAI_MODEL_DEPLOYMENT_NAME: ${VIA_VLM_OPENAI_MODEL_DEPLOYMENT_NAME:-}
      VILA_ENGINE_NGC_RESOURCE: ${VILA_ENGINE_NGC_RESOURCE:-}
      VILA_FORCE_ENGINE_BUILD: ${VILA_FORCE_ENGINE_BUILD:-false}
      VILA_LORA_PATH: ${VILA_LORA_PATH:-}
      VLM_BATCH_SIZE: ${VLM_BATCH_SIZE:-}
      VLM_INPUT_HEIGHT: ${VLM_INPUT_HEIGHT:-}
      VLM_INPUT_WIDTH: ${VLM_INPUT_WIDTH:-}
      VLM_MODEL_TO_USE: ${VLM_MODEL_TO_USE:-openai-compat}
      VSS_EXTRA_ARGS: ${VSS_EXTRA_ARGS:-}
      VSS_LOG_LEVEL: ${VSS_LOG_LEVEL:-}
      VSS_RTSP_LATENCY: ${VSS_RTSP_LATENCY:-}
      VSS_RTSP_TIMEOUT: ${VSS_RTSP_TIMEOUT:-}
    extra_hosts: &id004
      host.docker.internal: host-gateway
    healthcheck: &id005
      interval: 10s
      retries: 30
      start_period: 300s
      test:
      - CMD
      - sh
      - -c
      - curl -f http://localhost:${BACKEND_PORT:-8100}/health/ready
      timeout: 5s
    hostname: via-server
    image: ${VIA_IMAGE:-nvcr.io/nvidia/blueprint/vss-engine:2.3.0}
    ipc: host
    ports: &id006
    - ${BACKEND_PORT:-8100}:${BACKEND_PORT:-8100}
    - ${FRONTEND_PORT:-9100}:${FRONTEND_PORT:-9100}
    profiles:
    - local-deployment-dual-gpu
    restart: always
    runtime: nvidia
    stdin_open: true
    tty: true
    ulimits: &id007
      memlock:
        hard: -1
        soft: -1
      stack: 67108864
    volumes: &id008
    - ${ASSET_STORAGE_DIR:-/dummy}${ASSET_STORAGE_DIR:+:/tmp/assets}
    - ${GUARDRAILS_CONFIG:-/dummy}${GUARDRAILS_CONFIG:+:/opt/nvidia/via/guardrails_config}
    - ${GRAPH_RAG_PROMPT_CONFIG:-/dummy}${GRAPH_RAG_PROMPT_CONFIG:+:/opt/nvidia/via/warehouse_graph_rag_config.yaml}
    - ${EXAMPLE_STREAMS_DIR:-/dummy}${EXAMPLE_STREAMS_DIR:+:/opt/nvidia/via/streams:ro}
    - ${MILVUS_DATA_DIR:-/dummy}${MILVUS_DATA_DIR:+:/root/.milvus.io/milvus-server/2.3.5}
    - ${MODEL_ROOT_DIR:-/dummy}${MODEL_ROOT_DIR:+:${MODEL_ROOT_DIR:-}}
    - ${NGC_MODEL_CACHE:-via-ngc-model-cache}:/root/.via/ngc_model_cache
    - ${TRT_ENGINE_PATH:-/dummy}${TRT_ENGINE_PATH:+:${TRT_ENGINE_PATH:-}}
    - ${GSAM_MODEL_ROOT_DIR:-/dummy}${GSAM_MODEL_ROOT_DIR:+:${GSAM_MODEL_ROOT_DIR:-}}
    - ${VIA_SRC_DIR:-/dummy}${VIA_SRC_DIR:+:/opt/nvidia/via:ro}
    - ${VIA_LOG_DIR:-/dummy}${VIA_LOG_DIR:+:/tmp/via-logs}
    - via-hf-cache:/tmp/huggingface
    - ${CV_PIPELINE_TRACKER_CONFIG:-/dummy}${CV_PIPELINE_TRACKER_CONFIG:+:/opt/nvidia/via/config/default_tracker_config.yml}
  via-server-quad-gpu:
    configs:
    - source: server_config_quad_gpu
      target: /opt/nvidia/via/default_config.yaml
    depends_on: *id001
    deploy: *id002
    environment: *id003
    extra_hosts: *id004
    healthcheck: *id005
    hostname: via-server
    image: ${VIA_IMAGE:-nvcr.io/nvidia/blueprint/vss-engine:2.3.0}
    ipc: host
    ports: *id006
    profiles:
    - local-deployment-quad-gpu
    restart: always
    runtime: nvidia
    stdin_open: true
    tty: true
    ulimits: *id007
    volumes: *id008
  via-server-single-gpu:
    configs:
    - source: server_config_single_gpu
      target: /opt/nvidia/via/default_config.yaml
    depends_on: *id001
    deploy: *id002
    environment: *id003
    extra_hosts: *id004
    healthcheck: *id005
    hostname: via-server
    image: ${VIA_IMAGE:-nvcr.io/nvidia/blueprint/vss-engine:2.3.0}
    ipc: host
    ports: *id006
    profiles:
    - local-deployment-single-gpu
    restart: always
    runtime: nvidia
    stdin_open: true
    tty: true
    ulimits: *id007
    volumes: *id008
volumes:
  via-hf-cache: null
  via-ngc-model-cache: null
  via-nim-cache: null
